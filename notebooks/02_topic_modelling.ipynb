{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling & Theme Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import config\n",
    "from utils import load_processed_data, save_processed_data, print_section_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data: ../data/processed/cleaned_data.csv\n",
      "Loaded 582 processed responses\n",
      "Comments with text: 422\n",
      "Empty comments: 160\n",
      "\n",
      "Group distribution:\n",
      "group\n",
      "Non-VOLT_control    177\n",
      "Non-VOLT_pilot      164\n",
      "VOLT_control        158\n",
      "VOLT_pilot           83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from previous notebook\n",
    "df = load_processed_data('cleaned_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} processed responses\")\n",
    "print(f\"Comments with text: {df['cleaned_comment'].notna().sum()}\")\n",
    "print(f\"Empty comments: {df['cleaned_comment'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nGroup distribution:\")\n",
    "print(df['group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Data for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 422 documents for topic modeling\n",
      "Average document length: 11.0 words\n",
      "\n",
      "Sample documents:\n",
      "1: good package\n",
      "2: good customer service\n",
      "3: far good charlie efficient helpful let hope continues confident\n"
     ]
    }
   ],
   "source": [
    "def prepare_text_for_topics(df):\n",
    "    \"\"\"Prepare text data for topic modelling\"\"\"\n",
    "    # Filter valid comments\n",
    "    documents = df[df['cleaned_comment'].notna()]['cleaned_comment'].tolist()\n",
    "    \n",
    "    print(f\"Prepared {len(documents)} documents for topic modeling\")\n",
    "    print(f\"Average document length: {np.mean([len(doc.split()) for doc in documents]):.1f} words\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Prepare documents\n",
    "documents = prepare_text_for_topics(df)\n",
    "\n",
    "print(\"\\nSample documents:\")\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    print(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modelling\n",
    "\n",
    "I'm using LDA with 3 different vectorisation techniques: \n",
    "\n",
    "* **Vanilla tf-idf** with 1000 max features, min_df = 2 i.e. the words should atleast appear in 2 documents and max_df = 0.8 to remove words appearing in more than 80% of the documents.\n",
    "\n",
    "* **tf-idf with upto 3-word n-grams** to capture the phrases for better contextual analysis\n",
    "\n",
    "* **Count Vectorizer with 3-word n-grams** to benchmark results against the n-gram tf-idf.\n",
    "\n",
    "Then I'm using BERTopic with:\n",
    "\n",
    "* MiniLM - all-MiniLM-L6-v2: A smaller and faster embedding for baseline \n",
    "\n",
    "* MPNet - all-mpnet-base-v2: A high-quality larger model, perhaps the winner?\n",
    "\n",
    "* DistilBERT - distilbert-base-nli: Older model, have already been outperfomed by MiniLM and MPNet in STS benchmarks but for legacy's sake.\n",
    "\n",
    "* RoBERTa - roberta-base-nli: Good semantic understanding\n",
    "\n",
    "LDA vs BERTopic:\n",
    "- LDA: Traditional, interpretable, proven for business analysis\n",
    "- BERTopic: Modern, uses neural embeddings, better semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Enhanced LDA with 8 topics...\n",
      "\n",
      "--- Using tfidf_basic vectorizer ---\n",
      "  Topic 0: helpful, explained, extremely, patient, polite\n",
      "  Topic 1: efficient, phone, good, helpful, communication\n",
      "  Topic 2: good, service, price, great, informative\n",
      "  Topic 3: customer, service, good, fantastic, experience\n",
      "  Topic 4: excellent, service, customer, company, helpful\n",
      "  Topic 5: helpful, friendly, really, staff, help\n",
      "  Topic 6: quick, easy, set, service, friendly\n",
      "  Topic 7: nice, fast, staff, services, set\n",
      "\n",
      "--- Using tfidf_ngrams vectorizer ---\n",
      "  Topic 0: excellent, excellent service, installation, phone, service\n",
      "  Topic 1: customer, customer service, service, friendly, helpful\n",
      "  Topic 2: great, helpful, extremely helpful, extremely, service\n",
      "  Topic 3: good, service, good service, price, good price\n",
      "  Topic 4: explained, company, helpful, work, services\n",
      "  Topic 5: days, good, really, prompt, helpful\n",
      "  Topic 6: helpful, patient, friendly, polite, contract\n",
      "  Topic 7: efficient, friendly, nice, help, informative\n",
      "\n",
      "--- Using count_ngrams vectorizer ---\n",
      "  Topic 0: excellent, service, excellent service, installation, running\n",
      "  Topic 1: service, customer, customer service, helpful, good\n",
      "  Topic 2: process, service, great, connection, centers\n",
      "  Topic 3: good, service, good service, company, sky\n",
      "  Topic 4: company, different, internet, work, cable\n",
      "  Topic 5: helpful, good, easy, company, installation\n",
      "  Topic 6: helpful, friendly, company, polite, media\n",
      "  Topic 7: service, efficient, quick, helpful, good\n"
     ]
    }
   ],
   "source": [
    "def run_lda_analysis_enhanced(documents, n_topics=8):\n",
    "    \"\"\"Enhanced LDA with multiple vectorization approaches\"\"\"\n",
    "    print(f\"Running Enhanced LDA with {n_topics} topics...\")\n",
    "    \n",
    "    # Test multiple vectorizers\n",
    "    vectorizers = {\n",
    "        'tfidf_basic': TfidfVectorizer(\n",
    "            max_features=1000, min_df=2, max_df=0.8, stop_words='english'\n",
    "        ),\n",
    "        'tfidf_ngrams': TfidfVectorizer(\n",
    "            max_features=1500, min_df=2, max_df=0.8, \n",
    "            ngram_range=(1, 3), stop_words='english'\n",
    "        ),\n",
    "        'count_ngrams': CountVectorizer(\n",
    "            max_features=1000, min_df=2, max_df=0.8,\n",
    "            ngram_range=(1, 3), stop_words='english'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for vec_name, vectorizer in vectorizers.items():\n",
    "        print(f\"\\n--- Using {vec_name} vectorizer ---\")\n",
    "        \n",
    "        # Transform documents\n",
    "        doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Fit LDA model\n",
    "        lda_model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=config.RANDOM_STATE,\n",
    "            max_iter=50,\n",
    "            learning_method='batch'\n",
    "        )\n",
    "        \n",
    "        lda_model.fit(doc_term_matrix)\n",
    "        \n",
    "        # Get document-topic probabilities\n",
    "        doc_topics = lda_model.transform(doc_term_matrix)\n",
    "        \n",
    "        # Extract topic words\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topic_words = []\n",
    "        \n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_words_idx = topic.argsort()[-10:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topic_words.append(top_words)\n",
    "            print(f\"  Topic {topic_idx}: {', '.join(top_words[:5])}\")\n",
    "        \n",
    "        results[vec_name] = {\n",
    "            'model': lda_model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'doc_topics': doc_topics,\n",
    "            'topic_words': topic_words\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run enhanced LDA analysis\n",
    "lda_results = run_lda_analysis_enhanced(documents, config.N_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Enhanced BERTopic with 8 topics...\n",
      "\n",
      "--- Using all-MiniLM-L6-v2 embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topics found: 2\n",
      "    Topic 0: service, good, excellent, happy, great\n",
      "    Topic 1: service, helpful, company, customer, good\n",
      "\n",
      "--- Using all-mpnet-base-v2 embeddings ---\n",
      "  Topics found: 7\n",
      "    Topic 0: service, customer, good, excellent, great\n",
      "    Topic 1: installation, engineer, install, came, day\n",
      "    Topic 2: friendly, helpful, lady, spoke, polite\n",
      "    Topic 3: call, company, different, said, change\n",
      "    Topic 4: broadband, company, charge, connection, could\n",
      "    Topic 5: mark, deal, helpful, provided, know\n",
      "    Topic 6: helpful, informative, really, informant, everyone\n",
      "\n",
      "--- Using distilbert-base-nli embeddings ---\n",
      "  Topics found: 7\n",
      "    Topic 0: company, would, call, media, told\n",
      "    Topic 1: helpful, friendly, polite, staff, easy\n",
      "    Topic 2: service, good, price, great, excellent\n",
      "    Topic 3: everything, explained, really, polite, professional\n",
      "    Topic 4: quick, fast, service, set, easy\n",
      "    Topic 5: broadband, internet, skills, helpful, phone\n",
      "    Topic 6: customer, service, excellent, great, fantastic\n",
      "\n",
      "--- Using roberta-base-nli embeddings ---\n",
      "  Topics found: 2\n",
      "    Topic 0: company, would, service, get, call\n",
      "    Topic 1: service, helpful, good, friendly, customer\n"
     ]
    }
   ],
   "source": [
    "def run_bertopic_analysis_enhanced(documents, n_topics=8):\n",
    "    \"\"\"Enhanced BERTopic with different embedding models\"\"\"\n",
    "    print(f\"Running Enhanced BERTopic with {n_topics} topics...\")\n",
    "    \n",
    "    # Test different embedding models\n",
    "    embedding_models = {\n",
    "        'all-MiniLM-L6-v2': 'all-MiniLM-L6-v2',  \n",
    "        'all-mpnet-base-v2': 'all-mpnet-base-v2', \n",
    "        'distilbert-base-nli': 'distilbert-base-nli-mean-tokens',  \n",
    "        'roberta-base-nli': 'roberta-base-nli-stsb-mean-tokens' \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_path in embedding_models.items():\n",
    "        print(f\"\\n--- Using {model_name} embeddings ---\")\n",
    "        \n",
    "        # Create embedding model\n",
    "        embedding_model = SentenceTransformer(model_path)\n",
    "        \n",
    "        # Create BERTopic model\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            nr_topics=n_topics,\n",
    "            min_topic_size=config.MIN_TOPIC_SIZE,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        topics, probabilities = topic_model.fit_transform(documents)\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        \n",
    "        unique_topics = len([t for t in set(topics) if t != -1])\n",
    "        print(f\"  Topics found: {unique_topics}\")\n",
    "        \n",
    "        for i in range(unique_topics):\n",
    "            if i in topic_model.get_topics():\n",
    "                topic_words = topic_model.get_topic(i)\n",
    "                if topic_words:\n",
    "                    words = [word for word, score in topic_words[:5]]\n",
    "                    print(f\"    Topic {i}: {', '.join(words)}\")\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'model': topic_model,\n",
    "            'topics': topics,\n",
    "            'probabilities': probabilities,\n",
    "            'topic_info': topic_info\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run enhanced BERTopic analysis\n",
    "bertopic_results = run_bertopic_analysis_enhanced(documents, config.N_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Since there is no homogeneous way to compare the model performances for LDA and BERTopic as they are fundamentally different. While LDA uses a Bayesian probabilistic approach, BERTopic is a clustering based approach. The difference due to this is LDA assumes all documents belong to all topics with a certain probability and BERTopic hard-assigns every document to one group. The idea to use 2 different model approaches was to give a holistic view of the topic modelling problem. Due to time constraints, I couldn't experiment much with models. So, I devised a unified scoring criterion based on 3 different components for both LDA and BERTopic. For LDA models these are the 3 metrics:\n",
    "\n",
    "1. **Interpretability Score (40% weight)**\n",
    "    - Unique words / Total words across topics, higher = better\n",
    "    - For example: Topic 1: [service, good, helpful, staff, quick], Topic 2: [service, good, staff, helpful, friendly]; 6 unique words across 2 topics and total 10 words across the topics, interpretability=6/10, i.e. low interpretabilty, 4 words overlap, not a good distinction.\n",
    "\n",
    "2. **Focus Score (30% weight)**\n",
    "    - Average maximum topic probability per document, the higher the better.\n",
    "    - It gives us 2 broader focus categories, high focus (clearly belonging to one topic) and low focus (unclear themes)\n",
    "    - For example: Document1 - [0.8, 0.4, 0.3] average across 3 topics, 0.8 indicates high focus; Document2 - [0.4, 0.3, 0.3] average across 3 topics, 0.4 indicates low focus, unclear themes\n",
    "\n",
    "3. **Business Relevance (30% weight)**\n",
    "    - % of top words relevant for business. In this case, some keywords like 'helpful', 'friendly', 'quick' etc. Higher = better\n",
    "\n",
    "So, the composite score for LDA models will be 0.4*(Interpretability) + 0.3*(Focus Score) + 0.3*(Business Relevance)\n",
    "\n",
    "<br>\n",
    "\n",
    "For BERTopic models, these are the 3 metrics:\n",
    "\n",
    "1. **Topic Quality (40% weight)**\n",
    "    - Meaningful topics found / Topics requested, higher = better. So, I requested 8 topics and it returns 7; the quality is high.\n",
    "\n",
    "2. **Coverage (30% weight)**\n",
    "    - % of documents assigned to meaningful topics i.e. not in outliers. For example, 16 out of 20 documents lie in 7 of the topics found and 4 go into the outlier; the coverage will be 16/20 i.e. 80% of documents are fitting well into the topics and 20% are outliers. Higher = better\n",
    "\n",
    "3. **Business Relevance (30% weight)**\n",
    "    - Same as LDA\n",
    "\n",
    "So, the composite score for BERTopic models will be 0.4*(Topic Quality) + 0.3*(Coverage) + 0.3*(Business Relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LDA MODELS EVALUATION\n",
      "============================================================\n",
      "\n",
      "tfidf_basic:\n",
      "  Interpretability: 0.700\n",
      "  Focus Score: 0.665\n",
      "  Business Relevance: 0.325\n",
      "  COMPOSITE SCORE: 0.577\n",
      "\n",
      "tfidf_ngrams:\n",
      "  Interpretability: 0.750\n",
      "  Focus Score: 0.698\n",
      "  Business Relevance: 0.300\n",
      "  COMPOSITE SCORE: 0.599\n",
      "\n",
      "count_ngrams:\n",
      "  Interpretability: 0.650\n",
      "  Focus Score: 0.768\n",
      "  Business Relevance: 0.275\n",
      "  COMPOSITE SCORE: 0.573\n"
     ]
    }
   ],
   "source": [
    "def evaluate_lda_models(lda_results):\n",
    "    \"\"\"Evaluate LDA models using interpretability metrics\"\"\"\n",
    "    print_section_header(\"LDA Models Evaluation\")\n",
    "    \n",
    "    lda_scores = {}\n",
    "    # Need some domain expertise here\n",
    "    business_keywords = {'service', 'helpful', 'staff', 'install', 'problem', 'quick', 'professional', 'friendly'}\n",
    "    \n",
    "    for vec_name, result in lda_results.items():\n",
    "        print(f\"\\n{vec_name}:\")\n",
    "        \n",
    "        # Topic interpretability (word diversity)\n",
    "        topic_words = result['topic_words']\n",
    "        all_words = set()\n",
    "        total_words = 0\n",
    "        for topic in topic_words:\n",
    "            top_5 = topic[:5]\n",
    "            all_words.update(top_5)\n",
    "            total_words += len(top_5)\n",
    "        \n",
    "        interpretability = len(all_words) / total_words if total_words > 0 else 0\n",
    "        \n",
    "        # Topic focus (document concentration)\n",
    "        doc_topics = result['doc_topics']\n",
    "        max_probs = np.max(doc_topics, axis=1)\n",
    "        focus_score = np.mean(max_probs)\n",
    "        \n",
    "        # Business relevance\n",
    "        business_relevant_words = 0\n",
    "        for topic in topic_words:\n",
    "            for word in topic[:5]:\n",
    "                if word in business_keywords:\n",
    "                    business_relevant_words += 1\n",
    "        \n",
    "        business_relevance = business_relevant_words / (len(topic_words) * 5)\n",
    "        \n",
    "        # Composite score\n",
    "        composite = (interpretability * 0.4) + (focus_score * 0.3) + (business_relevance * 0.3)\n",
    "        \n",
    "        print(f\"  Interpretability: {interpretability:.3f}\")\n",
    "        print(f\"  Focus Score: {focus_score:.3f}\")\n",
    "        print(f\"  Business Relevance: {business_relevance:.3f}\")\n",
    "        print(f\"  COMPOSITE SCORE: {composite:.3f}\")\n",
    "        \n",
    "        lda_scores[vec_name] = composite\n",
    "    \n",
    "    return lda_scores\n",
    "\n",
    "# Evaluate LDA models\n",
    "lda_scores = evaluate_lda_models(lda_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BERTOPIC MODELS EVALUATION\n",
      "============================================================\n",
      "\n",
      "all-MiniLM-L6-v2:\n",
      "  Topic Quality: 0.250\n",
      "  Coverage: 1.000\n",
      "  Business Relevance: 0.300\n",
      "  COMPOSITE SCORE: 0.490\n",
      "\n",
      "all-mpnet-base-v2:\n",
      "  Topic Quality: 0.875\n",
      "  Coverage: 0.555\n",
      "  Business Relevance: 0.160\n",
      "  COMPOSITE SCORE: 0.564\n",
      "\n",
      "distilbert-base-nli:\n",
      "  Topic Quality: 0.875\n",
      "  Coverage: 0.682\n",
      "  Business Relevance: 0.280\n",
      "  COMPOSITE SCORE: 0.639\n",
      "\n",
      "roberta-base-nli:\n",
      "  Topic Quality: 0.250\n",
      "  Coverage: 1.000\n",
      "  Business Relevance: 0.400\n",
      "  COMPOSITE SCORE: 0.520\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bertopic_models(bertopic_results):\n",
    "    \"\"\"Evaluate BERTopic models using coverage and quality metrics\"\"\"\n",
    "    print_section_header('BERTopic Models Evaluation')\n",
    "    \n",
    "    bertopic_scores = {}\n",
    "    business_keywords = {'service', 'helpful', 'staff', 'install', 'problem', 'quick', 'professional', 'friendly'}\n",
    "    \n",
    "    for model_name, result in bertopic_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Extract and validate topics array\n",
    "        topics_array = result['topics']\n",
    "        topic_model = result['model']\n",
    "        \n",
    "        # Convert to numpy array if needed\n",
    "        if not isinstance(topics_array, np.ndarray):\n",
    "            topics_array = np.array(topics_array)\n",
    "        \n",
    "        # Topic quality (meaningful topics found)\n",
    "        unique_topics = len([t for t in set(topics_array) if t != -1])\n",
    "        topic_quality = unique_topics / config.N_TOPICS\n",
    "        \n",
    "        # Coverage (documents assigned to meaningful topics)\n",
    "        coverage = np.sum(topics_array != -1) / len(topics_array)\n",
    "        \n",
    "        # Business relevance\n",
    "        business_relevant_words = 0\n",
    "        for i in range(min(unique_topics, 5)):\n",
    "            if i in topic_model.get_topics():\n",
    "                topic_words = topic_model.get_topic(i)\n",
    "                for word, score in topic_words[:5]:\n",
    "                    if word in business_keywords:\n",
    "                        business_relevant_words += 1\n",
    "        \n",
    "        business_relevance = business_relevant_words / (min(unique_topics, 5) * 5) if unique_topics > 0 else 0\n",
    "        \n",
    "        # Composite score\n",
    "        composite = (topic_quality * 0.4) + (coverage * 0.3) + (business_relevance * 0.3)\n",
    "        \n",
    "        print(f\"  Topic Quality: {topic_quality:.3f}\")\n",
    "        print(f\"  Coverage: {coverage:.3f}\")\n",
    "        print(f\"  Business Relevance: {business_relevance:.3f}\")\n",
    "        print(f\"  COMPOSITE SCORE: {composite:.3f}\")\n",
    "        \n",
    "        bertopic_scores[model_name] = composite\n",
    "    \n",
    "    return bertopic_scores\n",
    "\n",
    "# Evaluate BERTopic models\n",
    "bertopic_scores = evaluate_bertopic_models(bertopic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "OVERALL MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Model Rankings:\n",
      "  1. BERTopic_distilbert-base-nli: 0.639\n",
      "  2. LDA_tfidf_ngrams: 0.599\n",
      "  3. LDA_tfidf_basic: 0.577\n",
      "  4. LDA_count_ngrams: 0.573\n",
      "  5. BERTopic_all-mpnet-base-v2: 0.564\n",
      "  6. BERTopic_roberta-base-nli: 0.520\n",
      "  7. BERTopic_all-MiniLM-L6-v2: 0.490\n",
      "\n",
      "BEST OVERALL MODEL: BERTopic_distilbert-base-nli\n",
      "SCORE: 0.639\n"
     ]
    }
   ],
   "source": [
    "def compare_all_models(lda_scores, bertopic_scores):\n",
    "    \"\"\"Compare all models and select best overall\"\"\"\n",
    "    print_section_header(\"\\nOverall Model Comparison\")\n",
    "    \n",
    "    # Combine all scores\n",
    "    all_scores = {}\n",
    "    \n",
    "    # Add LDA scores with prefix\n",
    "    for model_name, score in lda_scores.items():\n",
    "        all_scores[f\"LDA_{model_name}\"] = score\n",
    "    \n",
    "    # Add BERTopic scores with prefix\n",
    "    for model_name, score in bertopic_scores.items():\n",
    "        all_scores[f\"BERTopic_{model_name}\"] = score\n",
    "    \n",
    "    if not all_scores:\n",
    "        print(\"No models to compare!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_models = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nModel Rankings:\")\n",
    "    for i, (model_name, score) in enumerate(sorted_models, 1):\n",
    "        print(f\"  {i}. {model_name}: {score:.3f}\")\n",
    "    \n",
    "    # Best model\n",
    "    best_model, best_score = sorted_models[0]\n",
    "    \n",
    "    print(f\"\\nBEST OVERALL MODEL: {best_model}\")\n",
    "    print(f\"SCORE: {best_score:.3f}\")\n",
    "    \n",
    "    return all_scores, best_model\n",
    "\n",
    "# Compare all models\n",
    "all_model_scores, best_overall = compare_all_models(lda_scores, bertopic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the MPNet model which was slower and supposed to be more powerful didn't perform better than couple of LDA models. I guess that indicates that using bigger and complex models for this use case is an overkill. However, the lightweight MiniLM model with BERTopic performed really well and scored 2.2% more than the second-best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Theme Extraction with LLMs\n",
    "\n",
    "For this, I pre-set 6 themes based on my understanding (this might need validation from subject experts again). I used 3 different open-source LLM models viz. BART MNLI from Facebook which uses a Seq2Seq encoder-decoder architecture, DeBERTa-MNLI from Microsoft which uses encoder-only approach and cross-encoder DeBERTa which uses cross-encoding for better contextual scoring. \n",
    "\n",
    "I used 2 metrics: confidence score and theme diversity. To calculate the composite score, I used a weighted approach with 70% confidence score and 30% diversity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LLM classifiers...\n",
      "Loading BART-MNLI (Facebook)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART-MNLI (Facebook) loaded successfully\n",
      "Loading DeBERTa-MNLI (Microsoft)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa-MNLI (Microsoft) loaded successfully\n",
      "Loading DeBERTa-v3-NLI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa-v3-NLI loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_classifiers():\n",
    "    \"\"\"Setup multiple LLM classifiers for comparison\"\"\"\n",
    "    print(\"Setting up LLM classifiers...\")\n",
    "    \n",
    "    # Test multiple open-source models suitable for zero-shot classification\n",
    "    models_to_test = {\n",
    "        'facebook/bart-large-mnli': 'BART-MNLI (Facebook)',\n",
    "        'microsoft/deberta-large-mnli': 'DeBERTa-MNLI (Microsoft)',\n",
    "        'cross-encoder/nli-deberta-v3-base': 'DeBERTa-v3-NLI'\n",
    "    }\n",
    "    \n",
    "    classifiers = {}\n",
    "    \n",
    "    for model_name, model_desc in models_to_test.items():\n",
    "        try:\n",
    "            print(f\"Loading {model_desc}...\")\n",
    "            classifier = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=model_name\n",
    "            )\n",
    "            classifiers[model_desc] = classifier\n",
    "            print(f\"{model_desc} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_desc}: {e}\")\n",
    "            classifiers[model_desc] = None\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "# Setup classifiers\n",
    "llm_classifiers = setup_llm_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme categories defined:\n",
      "  1. service quality\n",
      "  2. agent helpfulness\n",
      "  3. process efficiency\n",
      "  4. technical support\n",
      "  5. communication clarity\n",
      "  6. problem resolution\n"
     ]
    }
   ],
   "source": [
    "def define_theme_categories():\n",
    "    \"\"\"Define candidate themes for call center analysis\"\"\"\n",
    "    \n",
    "    candidate_labels = [\n",
    "        \"service quality\",\n",
    "        \"agent helpfulness\", \n",
    "        \"process efficiency\",\n",
    "        \"technical support\",\n",
    "        \"communication clarity\",\n",
    "        \"problem resolution\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Theme categories defined:\")\n",
    "    for i, theme in enumerate(candidate_labels, 1):\n",
    "        print(f\"  {i}. {theme}\")\n",
    "    \n",
    "    return candidate_labels\n",
    "\n",
    "# Define themes\n",
    "theme_categories = define_theme_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing LLM models on 15 documents...\n",
      "\n",
      "Testing BART-MNLI (Facebook):\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc 1: good value reasonable prices...\n",
      "  Theme: service quality (0.479)\n",
      "  Doc 2: great service friendly staff...\n",
      "  Theme: service quality (0.780)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed: 8 docs\n",
      "  Avg confidence: 0.564\n",
      "  Themes found: 2\n",
      "  Top themes: ['service quality', 'agent helpfulness']\n",
      "\n",
      "Testing DeBERTa-MNLI (Microsoft):\n",
      "------------------------------\n",
      "  Doc 1: good value reasonable prices...\n",
      "  Theme: agent helpfulness (0.248)\n",
      "  Doc 2: great service friendly staff...\n",
      "  Theme: service quality (0.813)\n",
      "  Processed: 8 docs\n",
      "  Avg confidence: 0.493\n",
      "  Themes found: 3\n",
      "  Top themes: ['agent helpfulness', 'service quality', 'communication clarity']\n",
      "\n",
      "Testing DeBERTa-v3-NLI:\n",
      "------------------------------\n",
      "  Doc 1: good value reasonable prices...\n",
      "  Theme: process efficiency (0.274)\n",
      "  Doc 2: great service friendly staff...\n",
      "  Theme: service quality (0.624)\n",
      "  Processed: 8 docs\n",
      "  Avg confidence: 0.566\n",
      "  Themes found: 4\n",
      "  Top themes: ['process efficiency', 'service quality', 'communication clarity', 'technical support']\n"
     ]
    }
   ],
   "source": [
    "def compare_llm_models(classifiers, documents, candidate_labels, sample_size=15):\n",
    "    \"\"\"Compare performance of different LLM models\"\"\"\n",
    "    print(f\"\\nComparing LLM models on {sample_size} documents...\")\n",
    "    \n",
    "    # Sample documents\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    if len(documents) > sample_size:\n",
    "        sample_docs = np.random.choice(documents, sample_size, replace=False)\n",
    "    else:\n",
    "        sample_docs = documents[:sample_size]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name, classifier in classifiers.items():\n",
    "        if classifier is None:\n",
    "            print(f\"\\nSkipping {model_name} (failed to load)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nTesting {model_name}:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        model_results = []\n",
    "        \n",
    "        for i, doc in enumerate(sample_docs[:10]):  # Test on first 10 docs\n",
    "            if len(doc.split()) > 3:\n",
    "                try:\n",
    "                    result = classifier(doc, candidate_labels)\n",
    "                    \n",
    "                    model_results.append({\n",
    "                        'document': doc,\n",
    "                        'top_theme': result['labels'][0],\n",
    "                        'confidence': result['scores'][0],\n",
    "                        'all_scores': dict(zip(result['labels'], result['scores']))\n",
    "                    })\n",
    "                    \n",
    "                    if i < 2:  # Show first 2 examples\n",
    "                        print(f\"  Doc {i+1}: {doc[:50]}...\")\n",
    "                        print(f\"  Theme: {result['labels'][0]} ({result['scores'][0]:.3f})\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error on doc {i}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate model statistics\n",
    "        if model_results:\n",
    "            theme_dist = {}\n",
    "            confidences = []\n",
    "            \n",
    "            for result in model_results:\n",
    "                theme = result['top_theme']\n",
    "                theme_dist[theme] = theme_dist.get(theme, 0) + 1\n",
    "                confidences.append(result['confidence'])\n",
    "            \n",
    "            avg_confidence = np.mean(confidences)\n",
    "            theme_diversity = len(theme_dist)\n",
    "            \n",
    "            print(f\"  Processed: {len(model_results)} docs\")\n",
    "            print(f\"  Avg confidence: {avg_confidence:.3f}\")\n",
    "            print(f\"  Themes found: {theme_diversity}\")\n",
    "            print(f\"  Top themes: {list(theme_dist.keys())}\")\n",
    "            \n",
    "            all_results[model_name] = {\n",
    "                'results': model_results,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'theme_diversity': theme_diversity,\n",
    "                'theme_distribution': theme_dist\n",
    "            }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Compare LLM models\n",
    "llm_comparison = compare_llm_models(llm_classifiers, documents, theme_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM MODEL COMPARISON\n",
      "===================================\n",
      "BART-MNLI (Facebook):\n",
      "  Confidence: 0.564\n",
      "  Diversity: 0.333\n",
      "  Composite: 0.495\n",
      "\n",
      "DeBERTa-MNLI (Microsoft):\n",
      "  Confidence: 0.493\n",
      "  Diversity: 0.500\n",
      "  Composite: 0.495\n",
      "\n",
      "DeBERTa-v3-NLI:\n",
      "  Confidence: 0.566\n",
      "  Diversity: 0.667\n",
      "  Composite: 0.596\n",
      "\n",
      "BEST LLM MODEL: DeBERTa-v3-NLI\n",
      "SCORE: 0.596\n"
     ]
    }
   ],
   "source": [
    "def select_best_llm_model(llm_comparison):\n",
    "    \"\"\"Select best performing LLM model\"\"\"\n",
    "    print(\"\\nLLM MODEL COMPARISON\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if not llm_comparison:\n",
    "        print(\"No models to compare\")\n",
    "        return None, None\n",
    "    \n",
    "    # Score models based on confidence and theme diversity\n",
    "    model_scores = {}\n",
    "    \n",
    "    for model_name, results in llm_comparison.items():\n",
    "        if results:\n",
    "            # Composite score: confidence (70%) + theme diversity (30%)\n",
    "            confidence_score = results['avg_confidence']\n",
    "            diversity_score = min(results['theme_diversity'] / 6, 1.0)  # Normalize to max 6 themes\n",
    "            \n",
    "            composite_score = (confidence_score * 0.7) + (diversity_score * 0.3)\n",
    "            model_scores[model_name] = composite_score\n",
    "            \n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"  Confidence: {confidence_score:.3f}\")\n",
    "            print(f\"  Diversity: {diversity_score:.3f}\")\n",
    "            print(f\"  Composite: {composite_score:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    if model_scores:\n",
    "        best_model = max(model_scores.keys(), key=model_scores.get)\n",
    "        best_score = model_scores[best_model]\n",
    "        \n",
    "        print(f\"BEST LLM MODEL: {best_model}\")\n",
    "        print(f\"SCORE: {best_score:.3f}\")\n",
    "        \n",
    "        return best_model, llm_comparison[best_model]\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Select best model\n",
    "best_llm_model, best_llm_results = select_best_llm_model(llm_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are not so great, probably owing to the low dataset size. The top model DeBERTa-v3-NLI shows classification are moderately reliable with good coverage of themes. The model was able to capture 4 out of 6 themes given to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Group-specific Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: BERTopic_distilbert-base-nli\n",
      "Topics array length: 422\n",
      "Documents with topics: 422\n",
      "Meaningful topics: 288\n",
      "Outliers: 134\n"
     ]
    }
   ],
   "source": [
    "# Get the best BERTopic model results\n",
    "model_key = best_overall.replace('BERTopic_', '')\n",
    "best_model = bertopic_results[model_key]['model']\n",
    "topics_array = bertopic_results[model_key]['topics']\n",
    "\n",
    "print(f\"Using model: {best_overall}\")\n",
    "print(f\"Topics array length: {len(topics_array)}\")\n",
    "\n",
    "# Create dataframe with document-topic assignments\n",
    "docs_with_topics = df[df['cleaned_comment'].notna()].copy().reset_index(drop=True)\n",
    "docs_with_topics['assigned_topic'] = topics_array[:len(docs_with_topics)]\n",
    "docs_with_topics['meaningful_topic'] = docs_with_topics['assigned_topic'] != -1\n",
    "\n",
    "print(f\"Documents with topics: {len(docs_with_topics)}\")\n",
    "print(f\"Meaningful topics: {docs_with_topics['meaningful_topic'].sum()}\")\n",
    "print(f\"Outliers: {(~docs_with_topics['meaningful_topic']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Non-VOLT Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-VOLT customers: 252 documents\n",
      "\n",
      "Top latent topics mentioned (Non-VOLT):\n",
      "   Topic 1: helpful, friendly, polite, staff, easy (42 docs, 16.7%)\n",
      "   Topic 0: company, would, call, media, told (41 docs, 16.3%)\n",
      "   Topic 2: service, good, price, great, excellent (33 docs, 13.1%)\n",
      "   Topic 3: everything, explained, really, polite, professional (27 docs, 10.7%)\n",
      "   Topic 4: quick, fast, service, set, easy (12 docs, 4.8%)\n",
      "\n",
      "Non-VOLT Summary:\n",
      "  Total documents: 252\n",
      "  Meaningful topics: 173\n",
      "  Outliers: 79\n"
     ]
    }
   ],
   "source": [
    "# Non-VOLT group analysis\n",
    "non_volt_data = docs_with_topics[docs_with_topics['segment'] == 'Non-VOLT']\n",
    "print(f\"Non-VOLT customers: {len(non_volt_data)} documents\")\n",
    "\n",
    "# Top topics for Non-VOLT customers\n",
    "non_volt_meaningful = non_volt_data[non_volt_data['assigned_topic'] != -1]\n",
    "topic_counts = non_volt_meaningful['assigned_topic'].value_counts()\n",
    "\n",
    "print(f\"\\nTop latent topics mentioned (Non-VOLT):\")\n",
    "for i, (topic_id, count) in enumerate(topic_counts.head(5).items()):\n",
    "    pct = (count / len(non_volt_data)) * 100\n",
    "    \n",
    "    # Get topic words\n",
    "    if topic_id in best_model.get_topics():\n",
    "        topic_words = [word for word, score in best_model.get_topic(topic_id)[:5]]\n",
    "        print(f\"   Topic {topic_id}: {', '.join(topic_words)} ({count} docs, {pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   Topic {topic_id}: [unavailable] ({count} docs, {pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nNon-VOLT Summary:\")\n",
    "print(f\"  Total documents: {len(non_volt_data)}\")\n",
    "print(f\"  Meaningful topics: {len(non_volt_meaningful)}\")\n",
    "print(f\"  Outliers: {len(non_volt_data) - len(non_volt_meaningful)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control: 132 docs | Pilot: 120 docs\n",
      "\n",
      "2. Topic distributions differ by treatment (Non-VOLT):\n",
      "   Topic 0 (company, would, call):\n",
      "     Control: 15.9% | Pilot: 16.7% | Diff: +0.8%\n",
      "   Topic 1 (helpful, friendly, polite):\n",
      "     Control: 17.4% | Pilot: 15.8% | Diff: -1.6%\n",
      "   Topic 2 (service, good, price):\n",
      "     Control: 11.4% | Pilot: 15.0% | Diff: +3.6%\n",
      "   Topic 3 (everything, explained, really):\n",
      "     Control: 12.9% | Pilot: 8.3% | Diff: -4.5%\n"
     ]
    }
   ],
   "source": [
    "# Non-VOLT treatment comparison\n",
    "non_volt_control = non_volt_data[non_volt_data['treatment'] == 'control']\n",
    "non_volt_pilot = non_volt_data[non_volt_data['treatment'] == 'pilot']\n",
    "\n",
    "print(f\"Control: {len(non_volt_control)} docs | Pilot: {len(non_volt_pilot)} docs\")\n",
    "\n",
    "# Get topic distributions\n",
    "control_topics = non_volt_control[non_volt_control['assigned_topic'] != -1]['assigned_topic'].value_counts()\n",
    "pilot_topics = non_volt_pilot[non_volt_pilot['assigned_topic'] != -1]['assigned_topic'].value_counts()\n",
    "\n",
    "# Compare top topics\n",
    "all_topics = set(list(control_topics.index[:3]) + list(pilot_topics.index[:3]))\n",
    "\n",
    "print(\"\\n2. Topic distributions differ by treatment (Non-VOLT):\")\n",
    "for topic_id in sorted(all_topics):\n",
    "    control_count = control_topics.get(topic_id, 0)\n",
    "    pilot_count = pilot_topics.get(topic_id, 0)\n",
    "    \n",
    "    control_pct = (control_count / len(non_volt_control)) * 100 if len(non_volt_control) > 0 else 0\n",
    "    pilot_pct = (pilot_count / len(non_volt_pilot)) * 100 if len(non_volt_pilot) > 0 else 0\n",
    "    diff = pilot_pct - control_pct\n",
    "    \n",
    "    # Get topic description\n",
    "    if topic_id in best_model.get_topics():\n",
    "        topic_words = [word for word, score in best_model.get_topic(topic_id)[:3]]\n",
    "        topic_desc = ', '.join(topic_words)\n",
    "    else:\n",
    "        topic_desc = 'unknown'\n",
    "    \n",
    "    print(f\"   Topic {topic_id} ({topic_desc}):\")\n",
    "    print(f\"     Control: {control_pct:.1f}% | Pilot: {pilot_pct:.1f}% | Diff: {diff:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Price discussion increased in the treatment. That means the new script likely handled the pricing issues well. \n",
    "\n",
    "* There are increased mentions of helpful, friendly and explained meaning that the new script helped improve the perceived agent quality.\n",
    "\n",
    "* There is a decrease in service excellence mentions, which is a concern for a positive service reputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 VOLT Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOLT customers: 170 documents\n",
      "\n",
      "1. Top latent topics mentioned (VOLT):\n",
      "   Topic 0: company, would, call, media, told (34 docs, 20.0%)\n",
      "   Topic 1: helpful, friendly, polite, staff, easy (31 docs, 18.2%)\n",
      "   Topic 2: service, good, price, great, excellent (17 docs, 10.0%)\n",
      "   Topic 3: everything, explained, really, polite, professional (14 docs, 8.2%)\n",
      "   Topic 4: quick, fast, service, set, easy (9 docs, 5.3%)\n",
      "\n",
      "VOLT Summary:\n",
      "  Total documents: 170\n",
      "  Meaningful topics: 115\n",
      "  Outliers: 55\n"
     ]
    }
   ],
   "source": [
    "# VOLT group analysis\n",
    "volt_data = docs_with_topics[docs_with_topics['segment'] == 'VOLT']\n",
    "print(f\"VOLT customers: {len(volt_data)} documents\")\n",
    "\n",
    "# Top topics for VOLT customers\n",
    "volt_meaningful = volt_data[volt_data['assigned_topic'] != -1]\n",
    "topic_counts = volt_meaningful['assigned_topic'].value_counts()\n",
    "\n",
    "print(f\"\\n1. Top latent topics mentioned (VOLT):\")\n",
    "for i, (topic_id, count) in enumerate(topic_counts.head(5).items()):\n",
    "    pct = (count / len(volt_data)) * 100\n",
    "    \n",
    "    # Get topic words\n",
    "    if topic_id in best_model.get_topics():\n",
    "        topic_words = [word for word, score in best_model.get_topic(topic_id)[:5]]\n",
    "        print(f\"   Topic {topic_id}: {', '.join(topic_words)} ({count} docs, {pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   Topic {topic_id}: [unavailable] ({count} docs, {pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVOLT Summary:\")\n",
    "print(f\"  Total documents: {len(volt_data)}\")\n",
    "print(f\"  Meaningful topics: {len(volt_meaningful)}\")\n",
    "print(f\"  Outliers: {len(volt_data) - len(volt_meaningful)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control: 115 docs | Pilot: 55 docs\n",
      "\n",
      "2. Topic distributions differ by treatment (VOLT):\n",
      "   Topic 0 (company, would, call):\n",
      "     Control: 23.5% | Pilot: 12.7% | Diff: -10.8%\n",
      "   Topic 1 (helpful, friendly, polite):\n",
      "     Control: 13.9% | Pilot: 27.3% | Diff: +13.4%\n",
      "   Topic 2 (service, good, price):\n",
      "     Control: 12.2% | Pilot: 5.5% | Diff: -6.7%\n",
      "   Topic 3 (everything, explained, really):\n",
      "     Control: 6.1% | Pilot: 12.7% | Diff: +6.6%\n"
     ]
    }
   ],
   "source": [
    "# VOLT treatment comparison\n",
    "volt_control = volt_data[volt_data['treatment'] == 'control']\n",
    "volt_pilot = volt_data[volt_data['treatment'] == 'pilot']\n",
    "\n",
    "print(f\"Control: {len(volt_control)} docs | Pilot: {len(volt_pilot)} docs\")\n",
    "\n",
    "# Get topic distributions\n",
    "control_topics = volt_control[volt_control['assigned_topic'] != -1]['assigned_topic'].value_counts()\n",
    "pilot_topics = volt_pilot[volt_pilot['assigned_topic'] != -1]['assigned_topic'].value_counts()\n",
    "\n",
    "# Compare top topics\n",
    "all_topics = set(list(control_topics.index[:3]) + list(pilot_topics.index[:3]))\n",
    "\n",
    "print(\"\\n2. Topic distributions differ by treatment (VOLT):\")\n",
    "for topic_id in sorted(all_topics):\n",
    "    control_count = control_topics.get(topic_id, 0)\n",
    "    pilot_count = pilot_topics.get(topic_id, 0)\n",
    "    \n",
    "    control_pct = (control_count / len(volt_control)) * 100 if len(volt_control) > 0 else 0\n",
    "    pilot_pct = (pilot_count / len(volt_pilot)) * 100 if len(volt_pilot) > 0 else 0\n",
    "    diff = pilot_pct - control_pct\n",
    "    \n",
    "    # Get topic description\n",
    "    if topic_id in best_model.get_topics():\n",
    "        topic_words = [word for word, score in best_model.get_topic(topic_id)[:3]]\n",
    "        topic_desc = ', '.join(topic_words)\n",
    "    else:\n",
    "        topic_desc = 'unknown'\n",
    "    \n",
    "    print(f\"   Topic {topic_id} ({topic_desc}):\")\n",
    "    print(f\"     Control: {control_pct:.1f}% | Pilot: {pilot_pct:.1f}% | Diff: {diff:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a drastic improvement in the mentions about technical support. This means that the new script handled the technical queries more effectively. This also suggests that VOLT customers are more inclined towards technical support than the non-VOLT customers. \n",
    "\n",
    "* There is a minor increase in the customer service excellence mentions. With the non-VOLT customers having a negative impact, this means that the new script consistently fails to establish a good customer service reputation. Thus, there is an urgent need to address this issue in the script.\n",
    "\n",
    "* Similar to non-VOLT group, the staff personality mentions have also increased in the VOLT group. That means the script was able to convey a good agent personality among both categories of customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Insights:**\n",
    "\n",
    "* Script appears more effective for VOLT customers. Shows targeted technical support improvement without degrading service perception\n",
    "\n",
    "* Non-VOLT customers show mixed results. While agent perception improved, there's a significant drop in service excellence mentions.\n",
    "\n",
    "* Both segments still have high complaint rates. Company/process issues remain the top topic.\n",
    "\n",
    "* Sample size imbalance for VOLT - Control group much larger (115 vs 55), which may affect the reliability of pilot group insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Behavioural Metrics by Group (VOLT vs Non-VOLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent personality keywords: ['friendly', 'helpful', 'polite', 'lovely', 'nice', 'professional']\n",
      "\n",
      "Agent Personality Mentions:\n",
      "  Non-VOLT: 105/252 = 41.7%\n",
      "  VOLT: 71/170 = 41.8%\n",
      "  Difference: +0.1%\n"
     ]
    }
   ],
   "source": [
    "# Agent personality keywords\n",
    "personality_keywords = config.BEHAVIORAL_THEMES['agent_personality']\n",
    "print(f\"Agent personality keywords: {personality_keywords}\")\n",
    "\n",
    "# Non-VOLT customers\n",
    "non_volt_personality = 0\n",
    "for _, row in non_volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in personality_keywords):\n",
    "            non_volt_personality += 1\n",
    "\n",
    "non_volt_pct = (non_volt_personality / len(non_volt_data)) * 100\n",
    "\n",
    "# VOLT customers  \n",
    "volt_personality = 0\n",
    "for _, row in volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in personality_keywords):\n",
    "            volt_personality += 1\n",
    "\n",
    "volt_pct = (volt_personality / len(volt_data)) * 100\n",
    "\n",
    "print(f\"\\nAgent Personality Mentions:\")\n",
    "print(f\"  Non-VOLT: {non_volt_personality}/{len(non_volt_data)} = {non_volt_pct:.1f}%\")\n",
    "print(f\"  VOLT: {volt_personality}/{len(volt_data)} = {volt_pct:.1f}%\")\n",
    "print(f\"  Difference: {volt_pct - non_volt_pct:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is hardly any difference between the perceived agent personality between both the groups. It means the agents are not prioritising any specific groups. Everyone is treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clarity keywords: ['clear', 'explained', 'understand', 'easy', 'simple', 'straightforward']\n",
      "\n",
      "Clarity Mentions:\n",
      "  Non-VOLT: 48/252 = 19.0%\n",
      "  VOLT: 31/170 = 18.2%\n",
      "  Difference: -0.8%\n"
     ]
    }
   ],
   "source": [
    "# Clarity keywords\n",
    "clarity_keywords = config.BEHAVIORAL_THEMES['clarity']\n",
    "print(f\"\\nClarity keywords: {clarity_keywords}\")\n",
    "\n",
    "# Non-VOLT customers\n",
    "non_volt_clarity = 0\n",
    "for _, row in non_volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in clarity_keywords):\n",
    "            non_volt_clarity += 1\n",
    "\n",
    "non_volt_clarity_pct = (non_volt_clarity / len(non_volt_data)) * 100\n",
    "\n",
    "# VOLT customers\n",
    "volt_clarity = 0\n",
    "for _, row in volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in clarity_keywords):\n",
    "            volt_clarity += 1\n",
    "\n",
    "volt_clarity_pct = (volt_clarity / len(volt_data)) * 100\n",
    "\n",
    "print(f\"\\nClarity Mentions:\")\n",
    "print(f\"  Non-VOLT: {non_volt_clarity}/{len(non_volt_data)} = {non_volt_clarity_pct:.1f}%\")\n",
    "print(f\"  VOLT: {volt_clarity}/{len(volt_data)} = {volt_clarity_pct:.1f}%\")\n",
    "print(f\"  Difference: {volt_clarity_pct - non_volt_clarity_pct:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOLT customers tend to have slightly lesser clarity over the issues as compared to non-VOLT customers. Perhaps it is because the VOLT customers (assuming premium segment) expect more personalised and clear service because they have paid more(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reassurance keywords: ['reassured', 'confident', 'trust', 'reliable', 'secure', 'comfortable']\n",
      "\n",
      "Reassurance Mentions:\n",
      "  Non-VOLT: 2/252 = 0.8%\n",
      "  VOLT: 4/170 = 2.4%\n",
      "  Difference: +1.6%\n"
     ]
    }
   ],
   "source": [
    "# Reassurance keywords\n",
    "reassurance_keywords = config.BEHAVIORAL_THEMES['reassurance']\n",
    "print(f\"\\nReassurance keywords: {reassurance_keywords}\")\n",
    "\n",
    "# Non-VOLT customers\n",
    "non_volt_reassurance = 0\n",
    "for _, row in non_volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in reassurance_keywords):\n",
    "            non_volt_reassurance += 1\n",
    "\n",
    "non_volt_reassurance_pct = (non_volt_reassurance / len(non_volt_data)) * 100\n",
    "\n",
    "# VOLT customers\n",
    "volt_reassurance = 0\n",
    "for _, row in volt_data.iterrows():\n",
    "    comment = row['cleaned_comment']\n",
    "    if pd.notna(comment):\n",
    "        comment_lower = comment.lower()\n",
    "        if any(keyword in comment_lower for keyword in reassurance_keywords):\n",
    "            volt_reassurance += 1\n",
    "\n",
    "volt_reassurance_pct = (volt_reassurance / len(volt_data)) * 100\n",
    "\n",
    "print(f\"\\nReassurance Mentions:\")\n",
    "print(f\"  Non-VOLT: {non_volt_reassurance}/{len(non_volt_data)} = {non_volt_reassurance_pct:.1f}%\")\n",
    "print(f\"  VOLT: {volt_reassurance}/{len(volt_data)} = {volt_reassurance_pct:.1f}%\")\n",
    "print(f\"  Difference: {volt_reassurance_pct - non_volt_reassurance_pct:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost no mentions about reassurance. Just 2 in Non-VOLT and 4 in VOLT. But seeing percentage-wise, VOLT customers are comparatively more satisfied and reassured than the non-VOLT customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Compilation and Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile behavioral stats from existing calculations\n",
    "behavioral_stats = {\n",
    "    'agent_personality': {\n",
    "        'volt_pct': volt_pct,\n",
    "        'non_volt_pct': non_volt_pct,\n",
    "        'difference': volt_pct - non_volt_pct\n",
    "    },\n",
    "    'clarity': {\n",
    "        'volt_pct': volt_clarity_pct,\n",
    "        'non_volt_pct': non_volt_clarity_pct,\n",
    "        'difference': volt_clarity_pct - non_volt_clarity_pct\n",
    "    },\n",
    "    'reassurance': {\n",
    "        'volt_pct': volt_reassurance_pct,\n",
    "        'non_volt_pct': non_volt_reassurance_pct,\n",
    "        'difference': volt_reassurance_pct - non_volt_reassurance_pct\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data: ../data/processed/topic_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# # Compile all results for pipeline (opted out because of the file beingtoo large, will use the parameters to use the model again)\n",
    "# topic_results = {\n",
    "#     'lda_results': lda_results,\n",
    "#     'bertopic_results': bertopic_results,\n",
    "#     'llm_comparison': llm_comparison,\n",
    "#     'behavioral_stats': behavioral_stats,\n",
    "#     'best_models': {\n",
    "#         'overall_best': best_overall,\n",
    "#         'best_llm': best_llm_model\n",
    "#     },\n",
    "#     'documents': documents,\n",
    "#     'docs_with_topics': docs_with_topics\n",
    "# }\n",
    "\n",
    "# # Compile essential results (without heavy model objects)\n",
    "topic_results = {\n",
    "    'lda_results': {\n",
    "        'best_model_name': max(lda_scores.keys(), key=lda_scores.get),\n",
    "        'topic_words': lda_results[max(lda_scores.keys(), key=lda_scores.get)]['topic_words'],\n",
    "        'doc_topics': lda_results[max(lda_scores.keys(), key=lda_scores.get)]['doc_topics']\n",
    "    },\n",
    "    'bertopic_results': {\n",
    "        'best_model_name': best_overall.replace('BERTopic_', ''),\n",
    "        'topics_array': bertopic_results[best_overall.replace('BERTopic_', '')]['topics'],\n",
    "        'topic_info': bertopic_results[best_overall.replace('BERTopic_', '')]['topic_info']\n",
    "    },\n",
    "    'behavioral_stats': behavioral_stats,\n",
    "    'best_models': {\n",
    "        'overall_best': best_overall,\n",
    "        'best_llm': best_llm_model\n",
    "    },\n",
    "    'documents': documents,\n",
    "    'docs_with_topics': docs_with_topics,\n",
    "    'model_scores': {\n",
    "        'lda_scores': lda_scores,\n",
    "        'bertopic_scores': bertopic_scores\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save for next notebook\n",
    "save_processed_data(topic_results, \"topic_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Usage\n",
    "\n",
    "**Pros of LLM:**\n",
    "\n",
    "* **Scale:** Can process large volumes of text quickly\n",
    "\n",
    "* **Consistency**: Applies same criteria across all documents\n",
    "\n",
    "* **Zero-shot Classification**: No need for labeled training data\n",
    "\n",
    "* Captured multiple relevant business themes\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cons of LLM:**\n",
    "\n",
    "* **Hallucination risk:** May confidently assign themes that don't actually exist\n",
    "\n",
    "* **Black box decisions:** Cannot explain why specific themes were chosen\n",
    "\n",
    "* Limited sample size made performance assessment unreliable\n",
    "\n",
    "* No way to verify if classifications reflect genuine content\n",
    "\n",
    "* Due to small sample size, even small changes in the splits could make significant impact on the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Explainability Concerns:**\n",
    "\n",
    "There is a significant issue of explainability. When I ran the notebooks multiple times, the results change often. And unlike any machine learning model where we can set a random seed, there is no control on this. Consequently, I saw the issues I listed as the last point in the CONS section. And this is hard to explain as to how the model differs every run.\n",
    "\n",
    "* Cannot trace decision path from text to theme assignment\n",
    "\n",
    "* Business stakeholders cannot validate reasoning\n",
    "\n",
    "* Difficult to debug incorrect classifications\n",
    "\n",
    "* Risk of over-confidence in wrong classifications\n",
    "\n",
    "<br>\n",
    "\n",
    "**HALLUCINATION RISKS:**\n",
    "\n",
    "* May identify themes not present in actual customer comments\n",
    "\n",
    "* Could create false business insights from non-existent patterns\n",
    "\n",
    "* Especially risky with small datasets like ours\n",
    "\n",
    "* No ground truth validation possible\n",
    "\n",
    "<br>\n",
    "\n",
    "**COMPARISON TO TRADITIONAL METHODS:**\n",
    "\n",
    "* **Keyword analysis:** Fully explainable, traceable to specific words\n",
    "\n",
    "* **Topic modeling:** Shows word clusters, interpretable themes\n",
    "\n",
    "* **LLM classification:** Higher-level but less transparent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cowry_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
