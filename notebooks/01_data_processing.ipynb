{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Behaviorally-Optimised Call Script Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import config\n",
    "from utils import save_processed_data, print_section_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA LOADING AND PREPROCESSING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Data Loading and Preprocessing\")\n",
    "\n",
    "def load_and_combine_data(excel_path, control_sheet, pilot_sheet):\n",
    "    \"\"\"Load both sheets and combine with treatment labels\"\"\"\n",
    "    print(\"Loading Task 2 data...\")\n",
    "    \n",
    "    control_data = pd.read_excel(excel_path, sheet_name=control_sheet)\n",
    "    pilot_data = pd.read_excel(excel_path, sheet_name=pilot_sheet)\n",
    "    \n",
    "    control_data['treatment'] = 'control'\n",
    "    pilot_data['treatment'] = 'pilot'\n",
    "    \n",
    "    combined_data = pd.concat([control_data, pilot_data], ignore_index=True)\n",
    "    \n",
    "    # # Remove the original treatment-control indicator column\n",
    "    # combined_data.drop(columns=['COLUMN_4'], inplace=True)\n",
    "    \n",
    "    print(f\"Data loaded: {len(combined_data)} responses ({len(control_data)} control, {len(pilot_data)} pilot)\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Task 2 data...\n",
      "Data loaded: 647 responses (380 control, 267 pilot)\n",
      "Dataset shape: (647, 14)\n",
      "Columns: ['COLUMN_4', 'VOLT_FLAG', 'SURVEY_ID', 'SCORE', 'LTR_COMMENT', 'PRIMARY_REASON', 'TO_CHAR', 'CONNECTION_TIME', 'SALES_PERSON_SAT', 'SALES_FRIENDLY_SAT', 'COMMINICATION_SAT', 'FIRST_BILL_SAT', 'AGENT_KNOWLEDGE', 'treatment']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_4</th>\n",
       "      <th>VOLT_FLAG</th>\n",
       "      <th>SURVEY_ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>PRIMARY_REASON</th>\n",
       "      <th>TO_CHAR</th>\n",
       "      <th>CONNECTION_TIME</th>\n",
       "      <th>SALES_PERSON_SAT</th>\n",
       "      <th>SALES_FRIENDLY_SAT</th>\n",
       "      <th>COMMINICATION_SAT</th>\n",
       "      <th>FIRST_BILL_SAT</th>\n",
       "      <th>AGENT_KNOWLEDGE</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351124494</td>\n",
       "      <td>10</td>\n",
       "      <td>Conservations with your staff over the phone w...</td>\n",
       "      <td>Customer Service,Installation,Processes/Journe...</td>\n",
       "      <td>2023-02-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351727144</td>\n",
       "      <td>10</td>\n",
       "      <td>I spoke to tony and he was lovely, he talked m...</td>\n",
       "      <td>Brand,Changing ProviderEquipment,General Servi...</td>\n",
       "      <td>2023-02-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351645749</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351707395</td>\n",
       "      <td>10</td>\n",
       "      <td>Very positive so far. Cheaper, more included a...</td>\n",
       "      <td>General,Pricing,UK Legacy</td>\n",
       "      <td>2023-02-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351119086</td>\n",
       "      <td>10</td>\n",
       "      <td>They was on time. Done a good job. Was very po...</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>2023-02-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COLUMN_4 VOLT_FLAG  SURVEY_ID  SCORE  \\\n",
       "0  control       NaN  351124494     10   \n",
       "1  control       yes  351727144     10   \n",
       "2  control       yes  351645749     10   \n",
       "3  control       yes  351707395     10   \n",
       "4  control       NaN  351119086     10   \n",
       "\n",
       "                                         LTR_COMMENT  \\\n",
       "0  Conservations with your staff over the phone w...   \n",
       "1  I spoke to tony and he was lovely, he talked m...   \n",
       "2                                                NaN   \n",
       "3  Very positive so far. Cheaper, more included a...   \n",
       "4  They was on time. Done a good job. Was very po...   \n",
       "\n",
       "                                      PRIMARY_REASON              TO_CHAR  \\\n",
       "0  Customer Service,Installation,Processes/Journe...  2023-02-01 00:00:00   \n",
       "1  Brand,Changing ProviderEquipment,General Servi...  2023-02-01 00:00:00   \n",
       "2                                                NaN  2023-02-01 00:00:00   \n",
       "3                          General,Pricing,UK Legacy  2023-02-01 00:00:00   \n",
       "4                                   Customer Service  2023-02-01 00:00:00   \n",
       "\n",
       "   CONNECTION_TIME  SALES_PERSON_SAT  SALES_FRIENDLY_SAT  COMMINICATION_SAT  \\\n",
       "0             10.0              10.0                10.0               10.0   \n",
       "1             10.0               8.0                10.0               10.0   \n",
       "2             10.0              10.0                10.0               10.0   \n",
       "3             10.0               5.0                10.0               10.0   \n",
       "4             10.0               9.0                 5.0               10.0   \n",
       "\n",
       "   FIRST_BILL_SAT  AGENT_KNOWLEDGE treatment  \n",
       "0            10.0             10.0   control  \n",
       "1            10.0             10.0   control  \n",
       "2            10.0             10.0   control  \n",
       "3            10.0             10.0   control  \n",
       "4            10.0             10.0   control  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_and_combine_data(config.EXCEL_FILE, config.CONTROL_SHEET, config.PILOT_SHEET)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clean the `date` column\n",
    "\n",
    "I need to remove February data as required in the brief. Also, I see there are some messy dates like \"01/03/2company3\". I took the assumption that it is the year 2023 and the data was misplaced by replacing \"02\" from 2023 with \"company\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_date(excel_date):\n",
    "    \"\"\"Convert various date formats to datetime\"\"\"\n",
    "    if pd.isna(excel_date):\n",
    "        return None\n",
    "    \n",
    "    # Handle malicious text dates (like \"01/03/2company3\")\n",
    "    if isinstance(excel_date, str):\n",
    "        if \"company\" in excel_date.lower():\n",
    "            # Fix the malicious pattern: \"01/03/2company3\" -> \"01/03/2023\"\n",
    "            fixed_date = excel_date.replace(\"company\", \"02\")\n",
    "            try:\n",
    "                return pd.to_datetime(fixed_date, format='%m/%d/%Y')\n",
    "            except:\n",
    "                return None\n",
    "        # Try to parse other date strings normally\n",
    "        try:\n",
    "            return pd.to_datetime(excel_date, errors='coerce')\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Handle datetime objects (already converted by pandas)\n",
    "    if isinstance(excel_date, pd.Timestamp):\n",
    "        return excel_date\n",
    "    \n",
    "    # Handle normal Excel serial numbers\n",
    "    if isinstance(excel_date, (int, float)):\n",
    "        try:\n",
    "            return datetime(1900, 1, 1) + pd.Timedelta(days=excel_date-2)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Try direct conversion as fallback\n",
    "    try:\n",
    "        return pd.to_datetime(excel_date, errors='coerce')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_february_data(df):\n",
    "    \"\"\"Exclude February responses as required by brief\"\"\"\n",
    "    print(\"Applying temporal filtering...\")\n",
    "    \n",
    "    df['date_converted'] = df['TO_CHAR'].apply(convert_excel_date)\n",
    "    df['month'] = df['date_converted'].dt.month\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df_filtered = df[df['month'] != config.EXCLUDE_MONTH]\n",
    "    final_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"Excluded {initial_count - final_count} February responses\")\n",
    "    print(f\"Remaining: {final_count} responses\")\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying temporal filtering...\n",
      "Excluded 65 February responses\n",
      "Remaining: 582 responses\n",
      "\n",
      "Date range after filtering:\n",
      "Earliest: 2023-01-03 00:00:00\n",
      "Latest: 2023-06-01 00:00:00\n",
      "Valid months present: [1, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filter_february_data(df)\n",
    "\n",
    "print(\"\\nDate range after filtering:\")\n",
    "print(\"Earliest:\", df_filtered['date_converted'].min())\n",
    "print(\"Latest:\", df_filtered['date_converted'].max())\n",
    "print(\"Valid months present:\", sorted(df_filtered['month'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clean the text column `LTR_COMMENT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced text preprocessing using NLTK\"\"\"\n",
    "    if pd.isna(text) or text == 'NULL':\n",
    "        return None\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs and email addresses\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation but keep apostrophes for contractions\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Remove numbers unless they're part of words\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    # Simple tokenization (split by spaces instead of NLTK)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text if len(text) > 5 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid comments after cleaning: 422/582\n"
     ]
    }
   ],
   "source": [
    "df_filtered['cleaned_comment'] = df_filtered['LTR_COMMENT'].apply(clean_text)\n",
    "\n",
    "# Check text cleaning results\n",
    "total_comments = len(df_filtered)\n",
    "valid_comments = df_filtered['cleaned_comment'].notna().sum()\n",
    "print(f\"Valid comments after cleaning: {valid_comments}/{total_comments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create the VOLT and Non-VOLT segments\n",
    "\n",
    "By my understanding of data, I infer and assume VOLT is a \"premium\" user base, so I'll go with that assumption for the rest of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(df):\n",
    "    \"\"\"Create VOLT and Non-VOLT segments for each treatment\"\"\"\n",
    "    df['segment'] = df['VOLT_FLAG'].apply(lambda x: 'VOLT' if x == 'yes' else 'Non-VOLT')\n",
    "    df['group'] = df['segment'] + '_' + df['treatment']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Data Save and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset summary:\n",
      "Total responses: 582\n",
      "Comments with text: 422\n",
      "Empty comments: 160\n",
      "\n",
      "Segment distribution:\n",
      "group\n",
      "Non-VOLT_control    177\n",
      "Non-VOLT_pilot      164\n",
      "VOLT_control        158\n",
      "VOLT_pilot           83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final columns: ['treatment', 'segment', 'group', 'VOLT_FLAG', 'SCORE', 'cleaned_comment', 'date_converted', 'month', 'SALES_PERSON_SAT', 'SALES_FRIENDLY_SAT', 'COMMINICATION_SAT', 'FIRST_BILL_SAT', 'AGENT_KNOWLEDGE']\n",
      "Saved processed data: ../data/processed/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create segments\n",
    "df_final = create_segments(df_filtered)\n",
    "\n",
    "# Data quality checks\n",
    "print(\"Final dataset summary:\")\n",
    "print(f\"Total responses: {len(df_final)}\")\n",
    "print(f\"Comments with text: {df_final['cleaned_comment'].notna().sum()}\")\n",
    "print(f\"Empty comments: {df_final['cleaned_comment'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nSegment distribution:\")\n",
    "print(df_final['group'].value_counts())\n",
    "\n",
    "# Keep only essential columns\n",
    "columns_to_keep = [\n",
    "    'treatment', 'segment', 'group', 'VOLT_FLAG', 'SCORE', \n",
    "    'cleaned_comment', 'date_converted', 'month',\n",
    "    'SALES_PERSON_SAT', 'SALES_FRIENDLY_SAT', 'COMMINICATION_SAT', \n",
    "    'FIRST_BILL_SAT', 'AGENT_KNOWLEDGE'\n",
    "]\n",
    "\n",
    "df_final = df_final[columns_to_keep]\n",
    "print(f\"\\nFinal columns: {df_final.columns.tolist()}\")\n",
    "\n",
    "# Save for next notebook\n",
    "save_processed_data(df_final, \"cleaned_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cowry_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
